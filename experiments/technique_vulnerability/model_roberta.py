import os
from datasets import Dataset
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, multilabel_confusion_matrix
from transformers import (
    RobertaTokenizer,
    RobertaForSequenceClassification,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
import logging
import torch

from transformers import EvalPrediction
from transformers import logging as hf_logging

# Set the logging level for Transformers
hf_logging.set_verbosity_error()
logging.getLogger("transformers.trainer").setLevel(logging.ERROR)


class LoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        _ = logs.pop("total_flos", None)
        if state.is_local_process_zero:
            logging.info(f"Epoch: {int(state.epoch)}")
            logging.info(f"Best checkpoint: {state.best_model_checkpoint}, Best f1: {state.best_metric}")
            logging.info(logs)


class MultilabelTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = torch.nn.BCEWithLogitsLoss()
        loss = loss_fct(logits.view(-1, self.model.config.num_labels),
                        labels.float().view(-1, self.model.config.num_labels))
        return (loss, outputs) if return_outputs else loss


class RoBERTaModel:
    def __init__(self,
                 model,
                 max_length,
                 train_batch_size,
                 valid_batch_size,
                 epochs,
                 learning_rate,
                 output_dir,
                 num_labels):
        self.model_id = model
        self.max_length = max_length
        # config = AutoConfig.from_pretrained(self.model_id)
        # config.num_labels = num_labels
        # config.out_attentions = True
        # self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id,
        #                                                                 config=config)
        self.model = RobertaForSequenceClassification.from_pretrained(self.model_id,
                                                                      num_labels=num_labels)
        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_id,
                                                           do_lower_case=True,
                                                           max_length=self.max_length,  # input token size
                                                           device_map="auto")
        self.train_batch_size = train_batch_size
        self.valid_batch_size = valid_batch_size
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.output_dir = output_dir
        self.num_labels = num_labels
        self.train_data = None
        self.valid_data = None
        self.test_data = None

    def multi_label_metrics(self, predictions, labels, threshold=0.5):
        # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
        sigmoid = torch.nn.Sigmoid()
        probs = sigmoid(torch.Tensor(predictions))
        # next, use threshold to turn them into integer predictions
        y_pred = np.zeros(probs.shape)
        y_pred[np.where(probs >= threshold)] = 1
        # finally, compute metrics
        y_true = labels
        f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
        f1_macro_agerage = f1_score(y_true=y_true, y_pred=y_pred, average='macro')
        roc_auc = roc_auc_score(y_true, y_pred, average='micro')
        recall = recall_score(y_true, y_pred, average='micro')
        precision = precision_score(y_true, y_pred, average='micro')
        recall_each = recall_score(y_true, y_pred, average=None)
        precision_each = precision_score(y_true, y_pred, average=None)
        conf_matrix = multilabel_confusion_matrix(y_true, y_pred)

        logging.info(f"Recall each: {recall_each}")
        logging.info(f"Precision each: {precision_each}\n")
        accuracy = accuracy_score(y_true, y_pred)
        # return as dictionary
        metrics = {'f1': f1_micro_average,
                   'precision': precision,
                   'recall': recall,
                   'accuracy': accuracy,
                   'f1_micro': f1_micro_average,
                   'f1_macro': f1_macro_agerage,
                   'roc_auc': roc_auc,
                   'confusion_matrix': conf_matrix.tolist()}
        return metrics

    def compute_metrics(self, p: EvalPrediction):
        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
        result = self.multi_label_metrics(predictions=preds,
                                          labels=p.label_ids)
        return result

    def tokenize(self, batch):
        return self.tokenizer(batch["text"], padding=True, truncation=True)

    def convert_tech_to_label(self, tech_list_str):
        mapping = {"denial": 0,
                   "evasion": 1,
                   "feigning innocence": 2,
                   "rationalization": 3,
                   "playing victim role": 4,
                   "playing servant role": 5,
                   "shaming or belittlement": 6,
                   "intimidation": 7,
                   "brandishing anger": 8,
                   "accusation": 9,
                   "persuasion or seduction": 10}
        labels = [0.] * self.num_labels
        for tech in tech_list_str.lower().split(","):
            if tech in mapping:
                labels[mapping[tech]] = 1.
            else:
                print(f"Unknown technique: {tech_list_str}")
                exit(1)
        return labels

    def convert_vul_to_label(self, vul_list_str):
        mapping = {"naivete": 0,
                   "dependency": 1,
                   "over-responsibility": 2,
                   "over-intellectualization": 3,
                   "low self-esteem": 4}
        labels = [0.] * self.num_labels
        for vul in vul_list_str.lower().split(","):
            if vul in mapping:
                labels[mapping[vul]] = 1.
            else:
                print(f"Unknown vulnerability: {vul_list_str}")
                exit(1)
        return labels

    def finetuning(self, train_data, valid_data, test_data, measure):
        if measure == 'tech':
            train_data["label"] = train_data['Technique'].apply(self.convert_tech_to_label)
            valid_data["label"] = valid_data['Technique'].apply(self.convert_tech_to_label)
            test_data["label"] = test_data['Technique'].apply(self.convert_tech_to_label)
        else:
            train_data["label"] = train_data['Vulnerability'].apply(self.convert_vul_to_label)
            valid_data["label"] = valid_data['Vulnerability'].apply(self.convert_vul_to_label)
            test_data["label"] = test_data['Vulnerability'].apply(self.convert_vul_to_label)

        self.train_data = Dataset.from_pandas(train_data).rename_column("Dialogue", "text")
        self.valid_data = Dataset.from_pandas(valid_data).rename_column("Dialogue", "text")
        self.test_data = Dataset.from_pandas(test_data).rename_column("Dialogue", "text")

        train_dataset = self.train_data.map(self.tokenize, batched=True, batch_size=len(self.train_data),
                                            remove_columns=["text", "Manipulative", "Technique", "Vulnerability"])
        valid_dataset = self.valid_data.map(self.tokenize, batched=True, batch_size=len(self.valid_data),
                                            remove_columns=["text", "Manipulative", "Technique", "Vulnerability"])
        test_dataset = self.test_data.map(self.tokenize, batched=True, batch_size=len(self.test_data),
                                          remove_columns=["text", "Manipulative", "Technique", "Vulnerability"])

        # Set dataset format
        train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
        valid_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
        test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

        if os.path.exists(self.output_dir) is False:
            os.makedirs(self.output_dir)

        # TrainingArguments
        training_args = TrainingArguments(
            # The output directory where the model predictions and checkpoints will be written
            output_dir=self.output_dir,
            optim="adamw_torch",
            # The initial learning rate for AdamW optimizer
            learning_rate=self.learning_rate,
            # Total number of training epochs to perform
            num_train_epochs=self.epochs,
            # The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training
            per_device_train_batch_size=self.train_batch_size,
            # The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation
            per_device_eval_batch_size=self.valid_batch_size,
            # Evaluation is done at the end of each epoch. can be selected to {‘no’, ‘steps’, ‘epoch’}.
            evaluation_strategy="epoch",
            log_level="warning",
            logging_dir=f"logs",
            logging_strategy="no",
            disable_tqdm=False,
            # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.
            weight_decay=0.01,
            # Number of steps used for a linear warmup from 0 to learning_rate
            warmup_steps=500,
            # The checkpoint save strategy to adopt during training
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.
            save_total_limit=1
        )

        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=valid_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=self.compute_metrics,
            callbacks=[LoggingCallback]
        )

        # Fine-tune the model
        trainer.train()

        logging.info("")
        logging.info("-----Test-----")

        # Evaluate the model
        result = trainer.evaluate(test_dataset)
        for key, value in result.items():
            logging.info(f"{key} = {value}")
