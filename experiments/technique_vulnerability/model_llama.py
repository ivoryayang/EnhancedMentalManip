import logging

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TextIteratorStreamer,
    BitsAndBytesConfig,
    TrainingArguments,
    TrainerCallback
)
from threading import Thread
import torch
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
import os
from datasets import Dataset


class LoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        _ = logs.pop("total_flos", None)
        if state.is_local_process_zero:
            logging.info(f"Epoch: {int(state.epoch)}")
            logging.info(f"Best checkpoint: {state.best_model_checkpoint}, Best f1: {state.best_metric}")
            logging.info(logs)


class LlamaModel:
    def __init__(self,
                 model,
                 load_from_local,
                 temperature,
                 top_p,
                 top_k,
                 repetition_penalty,
                 max_new_tokens,
                 max_input_token_length,
                 ft_output_dir):
        if load_from_local:
            self.model_id = model
        else:
            self.model_id = "meta-llama/" + model
        self.model = AutoModelForCausalLM.from_pretrained(self.model_id,
                                                          torch_dtype=torch.float16,
                                                          device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, verbose=False)
        self.tokenizer.use_default_system_prompt = False
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.repetition_penalty = repetition_penalty
        self.max_new_tokens = max_new_tokens
        self.max_input_token_length = max_input_token_length
        self.ft_output_dir = ft_output_dir
        self.measure = None
        if not os.path.exists(self.ft_output_dir):
            os.makedirs(self.ft_output_dir)

    def zeroshot_prompting(self, dialogue, measure):
        conversation = []
        if measure == 'tech':
            system_prompt = ("Here are the definitions of 11 different mental manipulation techniques:\n"
                             "1. Denial: The manipulator either denies wrongdoing or pretends to be confused about others' concerns.\n"
                             "2. Evasion: The manipulator refuses to pay attention to something or gives irrelevant or vague responses.\n"
                             "3. Feigning Innocence: The manipulator implies that any harm caused was accidental.\n"
                             "4. Rationalization: The manipulator rationalizes their inappropriate behavior with excuses.\n"
                             "5. Playing the Victim Role: The manipulator portrays themselves as a victim to gain sympathy, attention, or divert focus from their misconduct.\n"
                             "6. Playing the Servant Role: The manipulator disguises their self-serving motives as a contribution to a more noble cause.\n"
                             "7. Shaming or Belittlement: The manipulator uses sarcasm, criticism, and put-downs to make others feel inferior, unworthy, or embarrassed.\n"
                             "8. Intimidation: The manipulator places others on the defensive by using veiled threats.\n"
                             "9. Brandishing Anger: The manipulator uses anger to brandish emotional intensity to shock the victim into submission.\n"
                             "10. Accusation: The manipulator suggests that the victim is at fault, selfish, uncaring, or leading an excessively easy life.\n"
                             "11. Persuasion or Seduction: The manipulator employs charm, emotional appeal, or logical reasoning to make others lower their defenses.\n\n"
                             "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                             "Please determine which manipulative techniques are used by the manipulator. "
                             "Respond only with the names of the techniques, and do not add anything else.\n\n")
        else:
            system_prompt = (
                "Here are the definitions of 5 different vulnerabilities targeted in victims of mental manipulation:\n"
                "1. Naivete: The victim is easily trusting and struggles to accept that others might be malevolent.\n"
                "2. Dependency: The victim has interest-based or emotional dependencies on the manipulator.\n"
                "3. Over-responsibility: The victim is overly self-critical and sets high standards for themselves, often assuming undue blame and responsibility for the manipulator's actions.\n"
                "4. Over-intellectualization: The victim rationalizes the manipulator's hurtful behavior by believing there is always a justified reason behind it.\n"
                "5. Low self-esteem: The victim is self-doubting and unconfident in pursuing their own wants and needs.\n\n"
                "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                "Please determine which vulnerabilities are targeted in the victim. "
                "Respond only with the names of the vulnerabilities, and do not add anything else.\n\n")

        conversation.append({"role": "system", "content": system_prompt})
        conversation.append({"role": "user", "content": dialogue})

        input_ids = self.tokenizer.apply_chat_template(conversation, return_tensors="pt")
        if input_ids.shape[1] > self.max_input_token_length:
            input_ids = input_ids[:, -self.max_input_token_length:]
        input_ids = input_ids.to(self.model.device)

        streamer = TextIteratorStreamer(self.tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = dict(
            {"input_ids": input_ids},
            streamer=streamer,
            max_new_tokens=self.max_new_tokens,  # generation length
            do_sample=False,
            top_p=self.top_p,
            top_k=self.top_k,
            temperature=self.temperature,
            num_beams=1,
            repetition_penalty=self.repetition_penalty,
        )

        t = Thread(target=self.model.generate, kwargs=generate_kwargs)
        t.start()

        outputs = []
        for text in streamer:
            outputs.append(text)
        res = ''.join(outputs)

        logging.info(system_prompt)
        logging.info(dialogue)
        logging.info('')
        logging.info(res)
        logging.info('')

        pred_list = []
        if measure == 'tech':
            if 'denial' in res.lower():
                pred_list.append("Denial")
            if "evasion" in res.lower():
                pred_list.append("Evasion")
            if "feigning innocence" in res.lower():
                pred_list.append("Feigning Innocence")
            if "rationalization" in res.lower():
                pred_list.append("Rationalization")
            if "playing the victim role" in res.lower():
                pred_list.append("Playing the Victim Role")
            if "playing the servant role" in res.lower():
                pred_list.append("Playing the Servant Role")
            if "shaming or belittlement" in res.lower():
                pred_list.append("Shaming or Belittlement")
            if "intimidation" in res.lower():
                pred_list.append("Intimidation")
            if "brandishing anger" in res.lower():
                pred_list.append("Brandishing Anger")
            if "accusation" in res.lower():
                pred_list.append("Accusation")
            if "persuasion or seduction" in res.lower():
                pred_list.append("Persuasion or Seduction")
        else:
            if 'naivete' in res.lower():
                pred_list.append("Naivete")
            if "dependency" in res.lower():
                pred_list.append("Dependency")
            if "over-responsibility" in res.lower():
                pred_list.append("Over-responsibility")
            if "over-intellectualization" in res.lower():
                pred_list.append("Over-intellectualization")
            if "low self-esteem" in res.lower():
                pred_list.append("Low self-esteem")

        return pred_list

    def fewshot_prompting(self, manip_examples, dialogue, measure):
        conversation = []
        total_example_num = len(manip_examples)
        if measure == 'tech':
            system_prompt = ("Here are the definitions of 11 different mental manipulation techniques:\n"
                             "1. Denial: The manipulator either denies wrongdoing or pretends to be confused about others' concerns.\n"
                             "2. Evasion: The manipulator refuses to pay attention to something or gives irrelevant or vague responses.\n"
                             "3. Feigning Innocence: The manipulator implies that any harm caused was accidental.\n"
                             "4. Rationalization: The manipulator rationalizes their inappropriate behavior with excuses.\n"
                             "5. Playing the Victim Role: The manipulator portrays themselves as a victim to gain sympathy, attention, or divert focus from their misconduct.\n"
                             "6. Playing the Servant Role: The manipulator disguises their self-serving motives as a contribution to a more noble cause.\n"
                             "7. Shaming or Belittlement: The manipulator uses sarcasm, criticism, and put-downs to make others feel inferior, unworthy, or embarrassed.\n"
                             "8. Intimidation: The manipulator places others on the defensive by using veiled threats.\n"
                             "9. Brandishing Anger: The manipulator uses anger to brandish emotional intensity to shock the victim into submission.\n"
                             "10. Accusation: The manipulator suggests that the victim is at fault, selfish, uncaring, or leading an excessively easy life.\n"
                             "11. Persuasion or Seduction: The manipulator employs charm, emotional appeal, or logical reasoning to make others lower their defenses.\n\n"
                             "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                             "Please determine which manipulative techniques are used by the manipulator. "
                             "Respond only with the names of the techniques, and do not add anything else. "
                             f"Here are {total_example_num} examples:\n\n")
        else:
            system_prompt = (
                "Here are the definitions of 5 different vulnerabilities targeted in victims of mental manipulation:\n"
                "1. Naivete: The victim is easily trusting and struggles to accept that others might be malevolent.\n"
                "2. Dependency: The victim has interest-based or emotional dependencies on the manipulator.\n"
                "3. Over-responsibility: The victim is overly self-critical and sets high standards for themselves, often assuming undue blame and responsibility for the manipulator's actions.\n"
                "4. Over-intellectualization: The victim rationalizes the manipulator's hurtful behavior by believing there is always a justified reason behind it.\n"
                "5. Low self-esteem: The victim is self-doubting and unconfident in pursuing their own wants and needs.\n\n"
                "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                "Please determine which vulnerabilities are targeted in the victim. "
                "Respond only with the names of the vulnerabilities, and do not add anything else. "
                f"Here are {total_example_num} examples:\n\n")

        conversation.append({"role": "system", "content": system_prompt})

        count_example = 0
        example_list = []
        for idx, row in manip_examples.iterrows():
            count_example += 1
            if measure == 'tech':
                row['Technique'] = row['Technique'].replace('Playing Victim Role', 'Playing the Victim Role')
                row['Technique'] = row['Technique'].replace('Playing Servant Role', 'Playing the Servant Role')
                example_answer = '\n'.join(row['Technique'].split(',')) + '\n'
            else:
                example_answer = '\n'.join(row['Vulnerability'].split(',')) + '\n'
            example = [
                {"role": "user", "content": f"Example {count_example}:\n{row['Dialogue']}"},
                {"role": "assistant", "content": example_answer},
            ]
            example_list.extend(example)

        conversation += example_list
        conversation.append({"role": "user", "content": dialogue})

        input_ids = self.tokenizer.apply_chat_template(conversation, return_tensors="pt")
        if input_ids.shape[1] > self.max_input_token_length:
            input_ids = input_ids[:, -self.max_input_token_length:]
        input_ids = input_ids.to(self.model.device)

        streamer = TextIteratorStreamer(self.tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = dict(
            {"input_ids": input_ids},
            streamer=streamer,
            max_new_tokens=self.max_new_tokens,  # generation length
            do_sample=False,
            top_p=self.top_p,
            top_k=self.top_k,
            temperature=self.temperature,
            num_beams=1,
            repetition_penalty=self.repetition_penalty,
        )

        t = Thread(target=self.model.generate, kwargs=generate_kwargs)
        t.start()

        outputs = []
        for text in streamer:
            outputs.append(text)
        res = ''.join(outputs)

        logging.info(system_prompt)
        for example in example_list:
            logging.info(example['content'])
        logging.info('')
        logging.info(dialogue)
        logging.info('')
        logging.info(res)
        logging.info('')

        pred_list = []
        if measure == 'tech':
            if 'denial' in res.lower():
                pred_list.append("Denial")
            if "evasion" in res.lower():
                pred_list.append("Evasion")
            if "feigning innocence" in res.lower():
                pred_list.append("Feigning Innocence")
            if "rationalization" in res.lower():
                pred_list.append("Rationalization")
            if "playing the victim role" in res.lower():
                pred_list.append("Playing the Victim Role")
            if "playing the servant role" in res.lower():
                pred_list.append("Playing the Servant Role")
            if "shaming or belittlement" in res.lower():
                pred_list.append("Shaming or Belittlement")
            if "intimidation" in res.lower():
                pred_list.append("Intimidation")
            if "brandishing anger" in res.lower():
                pred_list.append("Brandishing Anger")
            if "accusation" in res.lower():
                pred_list.append("Accusation")
            if "persuasion or seduction" in res.lower():
                pred_list.append("Persuasion or Seduction")
        else:
            if 'naivete' in res.lower():
                pred_list.append("Naivete")
            if "dependency" in res.lower():
                pred_list.append("Dependency")
            if "over-responsibility" in res.lower():
                pred_list.append("Over-responsibility")
            if "over-intellectualization" in res.lower():
                pred_list.append("Over-intellectualization")
            if "low self-esteem" in res.lower():
                pred_list.append("Low self-esteem")

        return pred_list

    def format_instruction(self, samples):
        formatted_samples = []
        if self.measure == 'tech':
            for i in range(len(samples['Technique'])):
                INSTRUCTION = "### Instruction:\n"
                INSTRUCTION += ("Here are the definitions of 11 different mental manipulation techniques:\n"
                                "1. Denial: The manipulator either denies wrongdoing or pretends to be confused about others' concerns.\n"
                                "2. Evasion: The manipulator refuses to pay attention to something or gives irrelevant or vague responses.\n"
                                "3. Feigning Innocence: The manipulator implies that any harm caused was accidental.\n"
                                "4. Rationalization: The manipulator rationalizes their inappropriate behavior with excuses.\n"
                                "5. Playing the Victim Role: The manipulator portrays themselves as a victim to gain sympathy, attention, or divert focus from their misconduct.\n"
                                "6. Playing the Servant Role: The manipulator disguises their self-serving motives as a contribution to a more noble cause.\n"
                                "7. Shaming or Belittlement: The manipulator uses sarcasm, criticism, and put-downs to make others feel inferior, unworthy, or embarrassed.\n"
                                "8. Intimidation: The manipulator places others on the defensive by using veiled threats.\n"
                                "9. Brandishing Anger: The manipulator uses anger to brandish emotional intensity to shock the victim into submission.\n"
                                "10. Accusation: The manipulator suggests that the victim is at fault, selfish, uncaring, or leading an excessively easy life.\n"
                                "11. Persuasion or Seduction: The manipulator employs charm, emotional appeal, or logical reasoning to make others lower their defenses.\n\n"
                                "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                                "Please determine which manipulative techniques are used by the manipulator. "
                                "Respond only with the names of the techniques, and do not add anything else.\n\n")
                INSTRUCTION += "### Dialogue: \n"
                INSTRUCTION += samples['Dialogue'][i]
                INSTRUCTION += "\n\n"
                RESPONSE_KEY = "### Answer: \n"

                samples['Technique'][i].replace('Playing Victim Role', 'Playing the Victim Role')
                samples['Technique'][i].replace('Playing Servant Role', 'Playing the Servant Role')
                tech_list = samples['Technique'][i].split(',')
                RESPONSE = '\n'.join(tech_list) + '\n'

                formatted_sample = INSTRUCTION + RESPONSE_KEY + RESPONSE
                formatted_samples.append(formatted_sample)
        else:
            for i in range(len(samples['Vulnerability'])):
                INSTRUCTION = "### Instruction:\n"
                INSTRUCTION += ("Here are the definitions of 5 different vulnerabilities targeted in victims of mental manipulation:\n"
                                "1. Naivete: The victim is easily trusting and struggles to accept that others might be malevolent.\n"
                                "2. Dependency: The victim has interest-based or emotional dependencies on the manipulator.\n"
                                "3. Over-responsibility: The victim is overly self-critical and sets high standards for themselves, often assuming undue blame and responsibility for the manipulator's actions.\n"
                                "4. Over-intellectualization: The victim rationalizes the manipulator's hurtful behavior by believing there is always a justified reason behind it.\n"
                                "5. Low self-esteem: The victim is self-doubting and unconfident in pursuing their own wants and needs.\n\n"
                                "Now, I will provide you with a dialogue that contains elements of mental manipulation. "
                                "Please determine which vulnerabilities are targeted in the victim. "
                                "Respond only with the names of the vulnerabilities, and do not add anything else.\n\n")
                INSTRUCTION += "### Dialogue: \n"
                INSTRUCTION += samples['Dialogue'][i]
                INSTRUCTION += "\n\n"
                RESPONSE_KEY = "### Answer: \n"

                vul_list = samples['Vulnerability'][i].split(',')
                RESPONSE = '\n'.join(vul_list) + '\n'

                formatted_sample = INSTRUCTION + RESPONSE_KEY + RESPONSE
                formatted_samples.append(formatted_sample)
        return formatted_samples

    def finetuning(self, train_data, valid_data, test_data, epochs, train_batch_size, lr, measure):
        self.measure = measure
        train_data = Dataset.from_pandas(train_data)
        valid_data = Dataset.from_pandas(valid_data)
        test_data = Dataset.from_pandas(test_data)

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        self.tokenizer.pad_token = self.tokenizer.eos_token  # </s>
        self.tokenizer.padding_side = "right"
        self.model = AutoModelForCausalLM.from_pretrained(self.model_id,
                                                          quantization_config=bnb_config,
                                                          trust_remote_code=True,
                                                          use_cache=False,
                                                          device_map="auto")
        self.model.config.pretraining_tp = 1

        # add "### Answer: \n" to tokenizer
        initial_token_count = len(self.tokenizer)
        instruction_template = "### Instruction:\n"
        dialogue_template = "### Dialogue: \n"
        response_template = "### Answer: \n"
        added_token_count = self.tokenizer.add_special_tokens({"additional_special_tokens": [instruction_template,
                                                                                             dialogue_template,
                                                                                             response_template]})
        self.model.resize_token_embeddings(new_num_tokens=initial_token_count + added_token_count)

        peft_config = LoraConfig(
            r=64,
            lora_alpha=16,
            lora_dropout=0.1,
            bias='none',
            task_type="CAUSAL_LM",
        )

        model = prepare_model_for_int8_training(self.model)
        model = get_peft_model(model, peft_config)

        collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,
                                                   response_template=response_template,
                                                   tokenizer=self.tokenizer)

        training_args = TrainingArguments(
            output_dir=self.ft_output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=train_batch_size,
            gradient_accumulation_steps=2,
            optim="paged_adamw_32bit",
            logging_steps=10,
            log_level="warning",
            logging_dir=f"logs",
            logging_strategy="steps",
            learning_rate=lr,
            fp16=True,
            tf32=False,
            max_grad_norm=0.3,
            warmup_ratio=0.03,
            lr_scheduler_type="constant",
            disable_tqdm=False,
            save_strategy="steps",
            save_total_limit=1
        )

        trainer = SFTTrainer(
            model=model,
            tokenizer=self.tokenizer,
            train_dataset=train_data,
            max_seq_length=4096,
            args=training_args,
            peft_config=peft_config,
            formatting_func=self.format_instruction,
            data_collator=collator,
            callbacks=[LoggingCallback],
        )

        trainer.train()

        model.save_pretrained(self.ft_output_dir)
        self.tokenizer.save_pretrained(self.ft_output_dir)
        logging.info(f"Fine-tuned {self.model_id} saved to {self.ft_output_dir}!")
